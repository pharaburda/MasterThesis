# -*- coding: utf-8 -*-
"""SynergicDeepModel(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DaHKRFAGns29oKcNgHAjyfiWe3FWPqSd
"""

from google.colab import drive
drive.mount('/content/drive')

pip install efficientnet_pytorch

import torch
import torchvision.models as models
from torch import nn, optim
import math
from torchvision import datasets
from torchvision.transforms import transforms
import os
from torchvision.utils import save_image
import torch.utils.data
import torch.nn.functional as F
from PIL import Image
import time
import re
import numpy as np
import matplotlib.pyplot as plt
import cv2
#from efficientnet_pytorch import EfficientNet

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
num_classes = 2
batch_size = 8
data_dir = '/content/drive/My Drive/glaucoma vs normal original/'

class SynergicNet(nn.Module):
    def __init__(self):
      super(SynergicNet, self).__init__()
      # self.fc1 = nn.Linear(175616, 1024)
      # self.fc2 = nn.Linear(1024, 256)
      # self.fc3 = nn.Linear(256, 2)
      self.fc1 = nn.Linear(4096, 1024)
      self.fc2 = nn.Linear(1024, 256)
      self.fc3 = nn.Linear(256, 2)

    def forward(self, first, second):
      x = torch.cat((first, second), 1).unsqueeze(1)
      x = torch.flatten(x, 1)
      #print(x.shape)
      x = self.fc1(x)
      x = self.fc2(x)
      x = self.fc3(x)
      output = F.softmax(x, dim=1)
      return output


def initialize_model(device, use_pretrained=True):
    model_1 = None
    model_2 = None
    torch.manual_seed(10)

    model_1 = models.resnet152(pretrained=use_pretrained)#.to(device)
    #model_1 = EfficientNet.from_pretrained('efficientnet-b4')
    model_2 = models.resnet152(pretrained=use_pretrained)#.to(device)
    #model_2 = EfficientNet.from_pretrained('efficientnet-b4')

    # num_ftrs = model_1.fc.in_features
    # model_1.fc = nn.Linear(num_ftrs, num_classes)

    # num_ftrs = model_2.fc.in_features
    # model_2.fc = nn.Linear(num_ftrs, num_classes)

    model_s = SynergicNet()#.to(device)
    return model_1, model_2, model_s

def save_checkpoint(epoch, model_c1, model_c2, model_cs, optimizer_c1,
                    optimizer_c2, optimizer_cs):
    path = "drive/My Drive/checkpoint.pt"
    torch.save({
            'epoch': epoch,
            'first_component_state_dict': model_c1.state_dict(),
            'second_component_state_dict': model_c2.state_dict(),
            'synergic_component_state_dict': model_cs.state_dict(),
            'optimizer_1_state_dict': optimizer_c1.state_dict(),
            'optimizer_2_state_dict': optimizer_c2.state_dict(),
            'optimizer_S_state_dict': optimizer_cs.state_dict()
            }, path)
    
def load_checkpoint(model_c1, model_c2, model_cs, optimizer_c1, optimizer_c2,
                    optimizer_cs):
    path = "drive/My Drive/checkpoint.pt"
    checkpoint = torch.load(path)
    epoch = checkpoint['epoch']
    model_c1.load_state_dict(checkpoint['first_component_state_dict'])
    model_c2.load_state_dict(checkpoint['second_component_state_dict'])
    model_cs.load_state_dict(checkpoint['synergic_component_state_dict'])
    optimizer_c1.load_state_dict(checkpoint['optimizer_1_state_dict'])
    optimizer_c2.load_state_dict(checkpoint['optimizer_2_state_dict'])
    optimizer_cs.load_state_dict(checkpoint['optimizer_S_state_dict'])

    return epoch

def load_checkpoint_models(model_c1, model_c2):
    path = "drive/My Drive/checkpoint.pt"
    checkpoint = torch.load(path)
    model_c1.load_state_dict(checkpoint['first_component_state_dict'])
    model_c2.load_state_dict(checkpoint['second_component_state_dict'])

def train_model(device, model_c1, model_c2, model_cs, dataloader_c1, dataloader_c2, 
                optimizer_c1, optimizer_c2, optimizer_cs,
                criterion_c1, criterion_c2, criterion_cs, num_epochs = 7,
                use_checkpoint = False):
    since = time.time()
    start_epoch = 0
    if (use_checkpoint):
        start_epoch = load_checkpoint(model_c1, model_c2, model_cs, optimizer_c1,
                                      optimizer_c2, optimizer_cs) + 1
    
    best_acc_1 = 0.0
    best_acc_2 = 0.0
    best_acc_s = 0.0

    for epoch in range(start_epoch, num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        for phase in ['train', 'val']:
            if phase == 'train':
                model_c1.train()
                model_c2.train()
                model_cs.train()
                
            else:
                model_c1.eval()
                model_c2.eval()
                model_cs.eval()
            
            running_loss_c1 = 0.0
            running_corrects_c1 = 0
            running_loss_c2 = 0.0
            running_corrects_c2 = 0
            running_loss_s = 0.0
            running_corrects_s = 0

            # todo rename into inputs and labels
            for (input_c1, label_c1), (input_c2, label_c2) in zip(dataloader_c1[phase], dataloader_c2[phase]):
                input_c1 = input_c1.to(device)
                label_c1 = label_c1.to(device)
                input_c2 = input_c2.to(device)
                label_c2 = label_c2.to(device)

                optimizer_c1.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs_c1 = model_c1(input_c1)
                    loss_c1 = criterion_c1(outputs_c1, label_c1)

                    _, preds_c1 = torch.max(outputs_c1, 1)

                    if phase == 'train':
                        loss_c1.backward()
                        optimizer_c1.step()
                  
                running_loss_c1 += loss_c1.item()
                running_corrects_c1 += torch.sum(preds_c1 == label_c1.data)
                  
                optimizer_c2.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs_c2 = model_c2(input_c2)
                    loss_c2 = criterion_c2(outputs_c2, label_c2)

                    _, preds_c2 = torch.max(outputs_c2, 1)

                    if phase == 'train':
                        loss_c2.backward()
                        optimizer_c2.step()

                running_loss_c2 += loss_c2.item()
                running_corrects_c2 += torch.sum(preds_c2 == label_c2.data)

                optimizerS.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    f_extract_c1 = torch.nn.Sequential(*list(model_c1.children())[:-1])
                    f_extract_c2 = torch.nn.Sequential(*list(model_c2.children())[:-1])
                    out_1 = f_extract_c1(input_c1)
                    out_2 = f_extract_c2(input_c2)
                    # out_1 = model_c1.extract_features(input_c1)
                    # out_2 = model_c2.extract_features(input_c2)
                    # print(out_1.shape)
                    # print(out_2.shape)
                
                    output = model_cs(out_1, out_2)
                    label = [1 if x==y else 0 for x, y in zip(label_c1, label_c2)]
                    label = torch.LongTensor(label).to(device)

                    lossS = criterion_cs(output, label)

                    _, preds_s = torch.max(output, 1)

                    if phase == 'train':
                        lossS.backward()
                        optimizerS.step()
               
                running_loss_s += lossS.item()
                running_corrects_s += torch.sum(preds_s == label.data)

            calculations_count = len(dataloader_c1[phase].dataset)

            epoch_loss = running_loss_c1 / calculations_count
            epoch_acc = running_corrects_c1.double() / calculations_count

            epoch_loss_2 = running_loss_c2 / calculations_count
            epoch_acc_2 = running_corrects_c2.double() / calculations_count

            epoch_loss_s = running_loss_s / calculations_count
            epoch_acc_s = running_corrects_s.double() / calculations_count

            print('{} First component loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))
            print('{} Second component loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss_2, epoch_acc_2))
            print('{} Synergic component loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss_s, epoch_acc_s))

            save_checkpoint(epoch, model_c1, model_c2, model_cs, optimizer_c1,
                    optimizer_c2, optimizer_cs)
            if phase == 'val' and epoch_acc > best_acc_1:
              best_acc_1 = epoch_acc

            if phase == 'val' and epoch_acc_2 > best_acc_2:
              best_acc_2 = epoch_acc_2
            
            if phase == 'val' and epoch_acc_s > best_acc_s:
              best_acc_s = epoch_acc_s

    print()
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Besc scores: first component {:.4f}, second component {:.4f} , synergic component {:.4f}'.format(best_acc_1, best_acc_2, best_acc_s))

class RandomMask(object):
    """Replaces black pixels with random ones."""

    def __call__(self, image):
      h = image.size[0]
      w = image.size[1]
      pixels = image.load()

      for y in range(0, h):
          for x in range(0, w):
              if (pixels[y,x]==(0,0,0)):
                r = np.random.randint(low = 0, high = 255)
                g = np.random.randint(low = 0, high = 255)
                b = np.random.randint(low = 0, high = 255)
                image.putpixel((y, x),(r, g, b))
      return image

class CLAHEHistogramEqualization(object):
  def __call__(self, image):
    image = np.array(image)
    image = np.uint8(image)

    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    lab_planes = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit = 60, tileGridSize = (16,16))
    lab_planes[0] = clahe.apply(lab_planes[0])
    lab = cv2.merge(lab_planes)
    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)

    image = Image.fromarray(bgr)
    return image

from torchvision.utils import save_image

input_size = (224, 224)
transform = transforms.Compose([
        transforms.Resize(input_size),
        #RandomMask(),
        CLAHEHistogramEqualization(),
        transforms.ToTensor(),
        transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
    ])

image_dataset = {x: datasets.ImageFolder(os.path.join(data_dir, x), transform) for x in ['train', 'val']}
#print(image_dataset['train'][0][0].shape)
#save_image(image_dataset['train'][0][0], 'image2.jpg')

def get_indices_for_class(data_folder):
    target = data_folder.targets
    label_indices = []

    for i in range(len(target)):
        if target[i] == AMD or target[i] == NORMAL:
            label_indices.append(i)

    np.random.shuffle(label_indices)
    return label_indices

#target_indices = {x: get_indices_for_class(image_dataset[x]) for x in ['train', 'val']}
#subsetRandomSampler = {x: torch.utils.data.sampler.SubsetRandomSampler(target_indices[x]) for x in ['train', 'val']}

imageloader_c1 = {x: torch.utils.data.DataLoader(image_dataset[x], shuffle=True, batch_size=batch_size, num_workers=8) for x in ['train', 'val']}
imageloader_c2 = {x: torch.utils.data.DataLoader(image_dataset[x], shuffle=True, batch_size=batch_size, num_workers=8) for x in ['train', 'val']}

model_c1, model_c2, model_cs = initialize_model(device)
model_c1 = model_c1.to(device)
model_c2 = model_c2.to(device)
model_cs = model_cs.to(device)

params_to_update = []
for param in model_c1.parameters():
  params_to_update.append(param)
for param in model_c2.parameters():
  params_to_update.append(param)
for param in model_cs.parameters():
  params_to_update.append(param)

optimizer1 = optim.SGD(model_c1.parameters(), lr=0.0003, momentum=0.9)
optimizer2 = optim.SGD(model_c2.parameters(), lr=0.0003, momentum=0.9)
optimizerS = optim.SGD(params_to_update, lr=0.0003, momentum=0.9)

criterion_c1 = nn.CrossEntropyLoss()
criterion_c2 = nn.CrossEntropyLoss()
synergic_criterion = nn.CrossEntropyLoss()

output = train_model(device, model_c1, model_c2, model_cs, imageloader_c1, imageloader_c2,
                     optimizer1,optimizer2,optimizerS, criterion_c1, criterion_c2,
                     synergic_criterion, num_epochs = 5, use_checkpoint = False)

print(*list(model_c2.children()))

!pip install dask

!pip install graphviz

!pip install toolz

from graphviz import Digraph
import torch
from torch.autograd import Variable

def make_dot(var, params=None):
    """ Produces Graphviz representation of PyTorch autograd graph
    Blue nodes are the Variables that require grad, orange are Tensors
    saved for backward in torch.autograd.Function
    Args:
        var: output Variable
        params: dict of (name, Variable) to add names to node that
            require grad (TODO: make optional)
    """
    if params is not None:
        assert isinstance(params.values()[0], Variable)
        param_map = {id(v): k for k, v in params.items()}

    node_attr = dict(style='filled',
                     shape='box',
                     align='left',
                     fontsize='12',
                     ranksep='0.1',
                     height='0.2')
    dot = Digraph(node_attr=node_attr, graph_attr=dict(size="12,12"))
    seen = set()

    def size_to_str(size):
        return '('+(', ').join(['%d' % v for v in size])+')'

    def add_nodes(var):
        if var not in seen:
            if torch.is_tensor(var):
                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')
            elif hasattr(var, 'variable'):
                u = var.variable
                name = param_map[id(u)] if params is not None else ''
                node_name = '%s\n %s' % (name, size_to_str(u.size()))
                dot.node(str(id(var)), node_name, fillcolor='lightblue')
            else:
                dot.node(str(id(var)), str(type(var).__name__))
            seen.add(var)
            if hasattr(var, 'next_functions'):
                for u in var.next_functions:
                    if u[0] is not None:
                        dot.edge(str(id(u[0])), str(id(var)))
                        add_nodes(u[0])
            if hasattr(var, 'saved_tensors'):
                for t in var.saved_tensors:
                    dot.edge(str(id(t)), str(id(var)))
                    add_nodes(t)
    add_nodes(var.grad_fn)
    return dot

make_dot(output).view()

from skimage import filters, exposure
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
from skimage.util import img_as_ubyte


def optic_disc_segmentation(path):
  image = cv2.imread(path)
  image = cv2.resize(image, (224, 224))
  #cv2.imwrite('image.jpg', image)

  #red_channel = image[:,:,2]

  filtered_img = filters.apply_hysteresis_threshold(image=image, low=180, high=200).astype(np.uint8)
  filtered_img = exposure.rescale_intensity(filtered_img, out_range=(0, 255)).astype(np.uint8)
  filtered_img = filtered_img * -255
  # cv2.imwrite('thresholded.png',filtered_img)
  #plt.imshow(filtered_img)

  filtered_img = img_as_ubyte(filtered_img)
  filtered_img = cv2.cvtColor(filtered_img, cv2.COLOR_BGR2GRAY)

  plt.figure()
  plt.imshow(filtered_img)

  dist_transform = cv2.distanceTransform(filtered_img,cv2.DIST_L2,5)
  maxDT = np.unravel_index(dist_transform.argmax(), dist_transform.shape)
  # cv2.circle(image, (maxDT[1], maxDT[0]),3,(0,0,255),-1)

  # plt.figure()
  # plt.imshow(dist_transform)

  h, w = filtered_img.shape
  mask = np.zeros((h + 2, w + 2), np.uint8)
  retval, filled_image, mask, rect	= cv2.floodFill(filtered_img, mask, (maxDT[1], maxDT[0]), 255)
  # plt.figure()
  # plt.imshow(filled_image)

  #print(rect)
  top_left_x = rect[0]
  top_left_y = rect[1]
  width = rect[2]
  height = rect[3]

  center = (top_left_x + width/2, top_left_y + height/2)
  r = int(cv2.norm((top_left_x, top_left_y), center))
  x = int(center[0])
  y = int(center[1])
  #print(center, r)
  # cv2.rectangle(image,rect,(0,0,255), 3,-1 )
  # plt.figure()
  # plt.imshow(image)


  # cnts = cv2.findContours(filled_image, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)

  # max_area = 0
  # max_area_idx = 0
  # for idx, contour in enumerate(cnts[0]):
  #   area = cv2.contourArea(contour)
  #   if area > max_area:
  #     max_area = area
  #     max_area_idx = idx

  # # # for contour in cnts[0]:
  # # #    cv2.drawContours(image, contour, -1, (0, 255, 0), 3)
  # cv2.drawContours(image, cnts[0][max_area_idx], -1, (0, 255, 0), 3)
  # plt.figure()
  # plt.imshow(image)

  # ######### todo największe koło, jakie da się wpisać w kontury ##########
  # #https://answers.opencv.org/question/204502/largest-circle-inside-a-contour-python/

  # try:
  #   (x,y), r = cv2.minEnclosingCircle(np.float32(cnts[0][max_area_idx]))
  # except:
  #   print("some error occured")
  #   return None
  x = np.uint8(x)
  y = np.uint8(y)
  r = np.uint8(2*r)
  if r > x:
    r = x - 1
  if r > y:
    r = y - 1

  crop_img = image[y-r:y+r, x-r:x+r]
  try:
    crop_img = cv2.resize(crop_img, (224, 224))
    crop_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)
  except:
    print('some error occured')
    print('y', y, 'x', x, 'r', r)
    return None

  return_image = Image.fromarray(crop_img)
  return return_image


def save_segmented_image(modify_dir, output_dir, suffix):
    image_dataset = datasets.ImageFolder(os.path.join(modify_dir))
    for file_path, file_class in image_dataset.imgs:
      if file_class == 0:
        file_name = os.path.splitext(os.path.basename(file_path))[0]
        segmented_image = optic_disc_segmentation(file_path)
        if segmented_image is not None:
          transform = transforms.ToTensor()
          save_image(transform(segmented_image), output_dir + file_name + suffix)
       
# glaucoma_dir = data_dir + 'test/'
# out_glaucoma_dir = '/content/drive/MyDrive/crop/test/glaucoma/'
# save_segmented_image(glaucoma_dir, out_glaucoma_dir , '_crop.jpg')

# test_dir = '/content/drive/MyDrive/temp/'
# out_dir = '/content/drive/MyDrive/out/'
# save_segmented_image(test_dir, out_dir , '_crop_2.jpg')

img_path = '/content/drive/MyDrive/glaucoma vs normal original/test/glaucoma/image42prime.tif'
seg_image = optic_disc_segmentation(img_path)
# transform = transforms.ToTensor()
# save_image(transform(seg_image), 'segmented image.png')

temp_img = '/content/drive/MyDrive/temp/temp/Kopia image45prime_2.jpg'
temp_dir = '/content/drive/MyDrive/temp'

transform = transforms.Compose([
        #CLAHEHistogramEqualization(),
        RandomMask(),
        transforms.ToTensor(),
    ])

image_dataset = datasets.ImageFolder(temp_dir, transform)
#print(image_dataset[0])
save_image(image_dataset[0][0], 'equalized image.png')

modify_dir = '/content/drive/My Drive/glaucoma central OD vs normal/train/'
output_dir = modify_dir + 'normal/'
NORMAL = 1
GLAUCOMA = 0
# AMD = 0
# DR = 2
# CATARACT = 1
flip_suffix = '_flip.jpg'
rotated_suffix = '_rotated.jpg'

input_size = (224, 224)
transform = transforms.Compose([
        transforms.Resize(input_size),
        transforms.ToTensor()
    ])
flipTransform = transforms.Compose([
        transforms.Resize(input_size),
        transforms.RandomHorizontalFlip(p=1),
        transforms.ToTensor()
    ])

rotationTransform = transforms.Compose([
        transforms.Resize(input_size),
        transforms.RandomRotation(degrees=10, resample=Image.BICUBIC),
        transforms.ToTensor()
    ])

cropTransform = transforms.Compose([
        transforms.Resize(input_size),
        transforms.CenterCrop((80, 80)),
        transforms.Resize(input_size),
        transforms.ToTensor()
    ])

normalizeTransform = transforms.Compose([
        transforms.Resize(input_size),
        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),
        transforms.ToTensor()
    ])

def save_transformed_image(_transform, _suffix):
    image_dataset = datasets.ImageFolder(os.path.join(modify_dir), _transform)
    for file_path, file_class in image_dataset.imgs:
      #print(file_path, file_class)
      if file_class == NORMAL:
        file_name = os.path.splitext(os.path.basename(file_path))[0]
        # file_extension = os.path.splitext(os.path.basename(file_path))[1]
        # no_suffix = file_extension
        idx = image_dataset.imgs.index((file_path, file_class))
        save_image(image_dataset[idx][0], output_dir + file_name + _suffix)

#save_transformed_image(flipTransform, flip_suffix)
#save_transformed_image(rotationTransform, rotated_suffix)
                      # can be also .tif extension
#save_transformed_image(cropTransform, ".jpg")

import os

phase = 'val'
#data_dir = '/content/drive/MyDrive/crop/'
 
glaucoma_dir = data_dir + phase + '/glaucoma'
glaucoma_len= len([name for name in os.listdir(glaucoma_dir) if os.path.isfile(os.path.join(glaucoma_dir, name))])
glaucoma_list = ['glaucoma'] * glaucoma_len
print(glaucoma_len) 

# retinopathy_dir = data_dir + phase + '/dr'
# retinopathy_len= len([name for name in os.listdir(retinopathy_dir) if os.path.isfile(os.path.join(retinopathy_dir, name))])
# retinopathy_list = ['dr'] * retinopathy_len
# print(retinopathy_len) 

# amd_dir = data_dir + phase + '/amd'
# amd_len= len([name for name in os.listdir(amd_dir) if os.path.isfile(os.path.join(amd_dir, name))])
# amd_list = ['amd'] * amd_len
# print(amd_len) 

normal_dir = data_dir + phase + '/normal'
normal_len= len([name for name in os.listdir(normal_dir) if os.path.isfile(os.path.join(normal_dir, name))])
normal_list = ['normal'] * normal_len
print(normal_len) 

# cataract_dir = data_dir + phase + '/cataract'
# cataract_len= len([name for name in os.listdir(cataract_dir) if os.path.isfile(os.path.join(cataract_dir, name))])
# cataract_list = ['cataract'] * cataract_len
# print(cataract_len)

# first_stage_dir = data_dir + phase + '/1'
# first_stage_len= len([name for name in os.listdir(first_stage_dir) if os.path.isfile(os.path.join(first_stage_dir, name))])
# first_stage_list = ['1'] * first_stage_len
# print(first_stage_len) 

# second_stage_dir = data_dir + phase + '/2'
# second_stage_len= len([name for name in os.listdir(second_stage_dir) if os.path.isfile(os.path.join(second_stage_dir, name))])
# second_stage_list = ['2'] * second_stage_len
# print(second_stage_len) 

# third_stage_dir = data_dir + phase + '/3'
# third_stage_len= len([name for name in os.listdir(third_stage_dir) if os.path.isfile(os.path.join(third_stage_dir, name))])
# third_stage_list = ['3'] * third_stage_len
# print(third_stage_len)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
evaluation_phase = 'val'

# model_c1, model_c2, model_cs = initialize_model(device)
# model_c1 = model_c1.to(device)
# model_c2 = model_c2.to(device)
# load_checkpoint_models(model_c1, model_c2)

def get_label_for_class(disease_class):
   return {
        0: 'glaucoma',
        1: 'normal',
        # 2: 'dr',
        # #1: 'glaucoma',
        # 3: 'normal'
    }[disease_class]

#y_test = np.concatenate([amd_list, retinopathy_list, glaucoma_list, normal_list])
y_test = np.concatenate([glaucoma_list, normal_list])

image_dataset = datasets.ImageFolder(os.path.join(data_dir, evaluation_phase), transform)
# target_indices = get_indices_for_class(image_dataset)
# subsetRandomSampler = torch.utils.data.sampler.SubsetRandomSampler(target_indices)

train_dataset = torch.utils.data.DataLoader(
        image_dataset, shuffle=False, num_workers=8
    )

model_c2.eval()
y_pred = []
pred_outputs = []

for i, (image, label) in enumerate(train_dataset, 0):
    image = image.to(device)
    outputs = model_c2(image)
    _, predicted = torch.max(outputs.data, 1)
    
    disease_class = predicted.cpu().numpy()[0]
    label_name = get_label_for_class(disease_class)
    y_pred.append(label_name)

    sample_fname, _ = train_dataset.dataset.samples[i]
    if disease_class != label and label == 0:
      pred_outputs.append(sample_fname + ' ' + label_name)


print(classification_report(y_pred, y_test))
#labels = ['amd', 'dr', 'glaucoma', 'normal']
labels = ['glaucoma', 'normal']
matrix = confusion_matrix(y_test, y_pred)
print(matrix)

ax= plt.subplot()
sns.heatmap(matrix, annot=True, ax=ax)

ax.set_xlabel('Predicted')
ax.set_ylabel('True')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)
plt.savefig('confusion matrix')

#print(pred_outputs)

import cv2
import numpy as np
import torch
from torchvision import models

class FeatureExtractor():
    """ Class for extracting activations and
    registering gradients from targetted intermediate layers """

    def __init__(self, model, target_layers):
        self.model = model
        self.target_layers = target_layers
        self.gradients = []

    def save_gradient(self, grad):
        self.gradients.append(grad)

    def __call__(self, x):
        outputs = []
        self.gradients = []
        for name, module in self.model._modules.items():
            x = module(x)
            if name in self.target_layers:
                x.register_hook(self.save_gradient)
                outputs += [x]
        return outputs, x


class ModelOutputs():
    """ Class for making a forward pass, and getting:
    1. The network output.
    2. Activations from intermeddiate targetted layers.
    3. Gradients from intermeddiate targetted layers. """

    def __init__(self, model, feature_module, target_layers):
        self.model = model
        self.feature_module = feature_module
        self.feature_extractor = FeatureExtractor(self.feature_module, target_layers)

    def get_gradients(self):
        return self.feature_extractor.gradients

    def __call__(self, x):
        target_activations = []
        for name, module in self.model._modules.items():
            if module == self.feature_module:
                target_activations, x = self.feature_extractor(x)
            elif "avgpool" in name.lower():
                x = module(x)
                x = x.view(x.size(0),-1)
            else:
                x = module(x)

        return target_activations, x


def preprocess_image(img):
    means = [0.485, 0.456, 0.406]
    stds = [0.229, 0.224, 0.225]

    preprocessed_img = img.copy()[:, :, ::-1]
    for i in range(3):
        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]
        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]
    preprocessed_img = \
        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))
    preprocessed_img = torch.from_numpy(preprocessed_img)
    preprocessed_img.unsqueeze_(0)
    input = preprocessed_img.requires_grad_(True)
    return input


def show_cam_on_image(img, mask):
    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)
    heatmap = np.float32(heatmap) / 255
    cam = heatmap + np.float32(img)
    cam = cam / np.max(cam)
    cv2.imwrite("cam.jpg", np.uint8(255 * cam))


class GradCam:
    def __init__(self, model, feature_module, target_layer_names, use_cuda):
        self.model = model
        self.feature_module = feature_module
        self.model.eval()
        self.cuda = use_cuda
        if self.cuda:
            self.model = model.cuda()

        self.extractor = ModelOutputs(self.model, self.feature_module, target_layer_names)

    def forward(self, input):
        return self.model(input)

    def __call__(self, input, index=None):
        if self.cuda:
            features, output = self.extractor(input.cuda())
        else:
            features, output = self.extractor(input)

        if index == None:
            index = np.argmax(output.cpu().data.numpy())

        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)
        one_hot[0][index] = 1
        one_hot = torch.from_numpy(one_hot).requires_grad_(True)
        if self.cuda:
            one_hot = torch.sum(one_hot.cuda() * output)
        else:
            one_hot = torch.sum(one_hot * output)

        self.feature_module.zero_grad()
        self.model.zero_grad()
        one_hot.backward(retain_graph=True)

        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()

        target = features[-1]
        target = target.cpu().data.numpy()[0, :]

        weights = np.mean(grads_val, axis=(2, 3))[0, :]
        cam = np.zeros(target.shape[1:], dtype=np.float32)

        for i, w in enumerate(weights):
            cam += w * target[i, :, :]

        cam = np.maximum(cam, 0)
        cam = cv2.resize(cam, input.shape[2:])
        cam = cam - np.min(cam)
        cam = cam / np.max(cam)
        return cam

def deprocess_image(img):
    """ see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 """
    img = img - np.mean(img)
    img = img / (np.std(img) + 1e-5)
    img = img * 0.1
    img = img + 0.5
    img = np.clip(img, 0, 1)
    return np.uint8(img*255)

#image_path = '/content/drive/My Drive/magisterka/val/glaucoma/glaucomaimage40prime_flip.jpg'
#image_path = '/content/drive/My Drive/magisterka/val/amd/aria_d_17_12.tif'
#image_path = '/content/drive/My Drive/magisterka/val/diabetic retinopathy/IDRiD_413_flip.jpg'
image_path = '/content/drive/MyDrive/glaucoma vs normal bigdataset/val/glaucoma/Im0483_g_ORIGA.jpg'
normal_path = '/content/drive/MyDrive/glaucoma vs normal bigdataset/val/normal/2401_left.jpg'

grad_cam = GradCam(model=model_c2, feature_module=model_c2.layer4, target_layer_names=["2"], use_cuda=True)

img = cv2.imread(normal_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = np.float32(cv2.resize(img, (224, 224))) / 255
input = preprocess_image(img)

# If None, returns the map for the highest scoring category.
# Otherwise, targets the requested index.
target_index = None
mask = grad_cam(input, target_index)

show_cam_on_image(img, mask)